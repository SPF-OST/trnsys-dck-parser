name: Run static checks and unit tests

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  test:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.12]

    steps:
    
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install wheel
        pip install -r requirements/dev.txt
    
    - name: Static type checking with mypy
      run: mypy --show-error-codes src test
    
    - name: Lint with pylint
      run: pylint test/test_trnsys_dck_parser src/trnsys_dck_parser
      
    - name: Get current time
      id: current_time
      run: echo "current_time=$(date -Iseconds)" >> "$GITHUB_OUTPUT"
      
    - name: Restore and save (at end of workflow) pytest-benchmark data
      id: pytest-benchmark
      uses: actions/cache@v4
      with:
        path: .benchmarks
        key: pytest-benchmark-${{steps.current_time.outputs.current_time}}
        restore-keys:
          pytest-benchmark-
    
    - name: Test with pytest
      shell: bash {0}
      run: |
        pytest test \
          --cov=trnsys_dck_parser \
          --cov-report=html:test-results/coverage \
          --cov-report=term \
          --cov-report=lcov:test-results/coverage.lcov \
          --html=test-results/report/report.html \
          --benchmark-disable
        
    - name: Benchmark with pytest-benchmark
      shell: bash {0}
      run: |
        compare_args=$(test -d .benchmarks && echo \
          --benchmark-compare \
          --benchmark-compare-fail=mean:5%) \
        pytest test --benchmark-autosave $compare_args
        
    - name: Create histograms
      shell: bash {0}
      run: |
        if [ -d .benchmarks ]; then
          py.test-benchmark compare --histogram='test-results/benchmark/histogram'
        else
          echo "No previous benchmarks found."
        fi

    - name: Post coverage report to Coveralls
      uses: coverallsapp/github-action@v2
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        path-to-lcov: test-results/coverage.lcov
        
    - name: Compute artifact file name
      id: fn
      run: |
        sha=$(git rev-parse --short HEAD)
        ref_name=$(echo "${{github.ref_name}}" | sed 's|/|-|g')
        echo "artifact_file_name=trnsys-dck-parser-artifacts-$sha-$ref_name" >> $GITHUB_OUTPUT
        
    - name: Upload test results and coverage reports
      uses: actions/upload-artifact@v4
      with:
        name: ${{steps.fn.outputs.artifact_file_name}}
        path: test-results
      # Use always() to always run this step to publish test results when there are test failures
      if: ${{ always() }}
